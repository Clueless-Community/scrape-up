# -*- coding: utf-8 -*-
"""git_scraper.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1vVkTt6t8CcCS4tXkAsfDVjD9BkoYyqPC
"""





import requests
from bs4 import BeautifulSoup
import webbrowser

def get_all_repo_details(username):
    url = f'https://github.com/{username}?tab=repositories'
    response = requests.get(url)

    if response.status_code != 200:
        print('Error: Failed to retrieve repository details')
        return []

    soup = BeautifulSoup(response.content, 'html.parser')

    repo_elements = soup.select('#user-repositories-list ul li')
    repositories = []

    for repo_element in repo_elements:
        #to find url
        link = repo_element.find('a')['href']

        #to find name
        name = repo_element.find('a').text.split('/')[-1]
        name= name.replace('\n', '')

        #to find description
        description = repo_element.find('p').get_text(strip=True) if repo_element.find('p') else None
        #description= description.replace('\n','')

        url1= 'https://github.com' + link
        response1= requests.get(url1)
        soup= BeautifulSoup(response1.content, 'html.parser')
        #to find language


        #languagedata= soup.select_one('span.color-text-primary.text-bold.mr-1')
        #language= languagedata.text.strip() if languagedata else 'N/A'

        li_elements = soup.find_all('li', class_= "d-inline")
        for li in li_elements:
          language = li.text.strip()
          language= language.replace('\n','')



       #to find num of stars
        stars= soup.find('span', class_='text-bold')
        num_of_stars = stars.text if stars else 'N/A'
        num_of_stars= num_of_stars.replace('\n','')



        pullurl= url1 + '/pulls'
        issuesurl= url1+'/issues'
        pullresponse= requests.get(pullurl) #getting the content of pull requests page
        issueresponse= requests.get(issuesurl) #getting the content of issues page

        p_soup= BeautifulSoup(pullresponse.content, 'html.parser') #creating beautifulsoup object for pull requests page
        i_soup= BeautifulSoup(issueresponse.content, 'html.parser') #creating beautifulsoup object for issues page

        #to find number of pull requests
        pullrequests= p_soup.find('div', class_="table-list-header-toggle states flex-auto pl-0" )
        num_of_pull_requests= pullrequests.text.strip() if pullrequests else 'N/A'
        num_of_pull_requests= num_of_pull_requests.replace('\n','')

        #to find number of issues

        issues= i_soup.select_one('span#issues-repo-tab-count')
        num_of_issues= issues.text.strip() if issues else 'N/A'
        num_of_issues= num_of_issues.replace('\n','')


        repository = {
            'name': name,
            'link': f'https://github.com{link}',
            'description': description,
            'Programming language': language,
            'Stars': num_of_stars,
            'Issues': num_of_issues,
            'number of pull requests(open and closed)': num_of_pull_requests,

        }

        repositories.append(repository)

    return repositories













